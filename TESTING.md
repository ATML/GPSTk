Testing of the GPSTk
--------------------

Introduction
------------
As an open source project, the source of the GPSTk is subject to intermittent updates, contributions, 
and corrections. The GPSTk testing process has been redesigned to build confidence in the 
functionality of the toolkit. Testing within the GPSTk is designed with the following distinct goals 
in mind:
	
* Testing is repeatable with a low amount of effort.
* Testing is distributed along with the source to support both internal testing and to assure outside users and contributors of the quality of the library.
* Testing is designed to accommodate easy additions to the existing test suite.
* Testing is implemented to ensure changes have not broken functionality.

All testing is performed using cmake/ctest. This allows the testing to function on all supported platforms.

The goal is to have some level of testing performed on all classes and applications in the GPSTk core. It is
encouraged that tests be written for all developed code whether it be in the core or ext. In general an 'entrance
criteria' for code to be included in the core is that it is tested and stable.


How to execute unit tests
-------------------------
1. Using build.sh
   * `$ cd ~/git/gpstk`
   * `$ build.sh -te`
1. Manually
   * `$ cd ~/git/gpstk/build`
   * `$ cmake .. -DTEST_SWITCH=ON`
   * `$ make`
   * `$ ctest`

How to debug the unit test results
----------------------------------
1. Run ctest with `-V` option or build.sh with the `-v` option
1. Examine the detailed log generated by ctest (does not require -V)
   * build/Testing/Temporary/LastTest.log

How to Write Class Unit Tests
-----------------------------
1. Write a C++ program in core/tests/... or ext/tests/...
   * File name starts with class name and ends with \_T before the ".cpp". For example, the test program for a class named foo would be called foo\_T.cpp.
1. Add any required data to the appropriate data directory. See Testing Data section for more detail.
1. Modify the CMakeLists.text to build and run the tests.

Current Unit Test Structure
---------------------------
* Unit tests for a particular GPSTk library class are organized in a single cpp file titled by 
  the GPSTk library class under test with a _T.cpp appended.
* Unit test files are kept in gpstk/core/tests and gpstk/ext/tests in the same subdirectories as in gpstk/core/lib/ and gpstk/ext/lib/.
* The individual cpp files are broken into two parts, a test class to test the GPSTk library class and 
  a main() segment to run those tests.
* The test class is organized into multiple public methods in which each method contains multiple 
  assertions which test a particular feature of the GPSTk library class under test.
* The test class might inherit from the GPSTk library class in order to access protected members for 
  direct checking of values.
* To facilitate reporting to the testing logs, GPSTk uses its own TestUtil class. 
  TestUtil provides standardized output containing information on the GPSTk library class being tested, 
  feature of class being tested, test file name, line number of test in that file, pass/fail bit, and 
  a failure message should the test have failed. It also provides the number of failures to the main() 
  portion of the test cpp file. The current style of using the TestUtil class is by use of its macros.
* The main() portion of the code creates the test class object and executes its methods. It then tallies 
  the number of failures and reports it to the screen/log.
* Data for testing is located in the gpstk/data directory. Only place data in there that is publicly releasable.
* The file build_config.h.in is configured by the cmake process to define some functions to allow C++ programs
to find this data after they are compiled.
* The CMAKE variable GPSTK\_TEST\_DATA\_DIR can be used to find the data from cmake. It is defined in the top level CMakeLists.txt file.

How to Write Application Tests
------------------------------
1. The application tests utilize CMake scripts to run the GPSTk applications with varying options and data.
1. The tests are added to the CMakeLists.txt file in the application's corresponding subdirectory of gpstk/core/tests/ or gpstk/ext/tests/.
1. When possible, utilize one of the shared .cmake files in gpstk/core/tests/ in order to perform tests. The shared .cmake files include the following functionality:
   * testhelp.cmake - Runs the application with various forms of help options to ensure they all work.
   * testfailexp.cmake - Runs the application with given options and expects that the application should exit with a code other than 0, but not segmentation fault.
   * testsuccexp.cmake - Runs the application with given options and expects that the application should exit with a code 0. It can also compare whole files.
   * testsuccdiff.cmake - Runs the application with given options and then diffs the output of the application with a stored expected output.
1. If the tested application use case does not fit into one of the above scripts, feel free to create a new one. Be sure to include why a new one was needed in the comments of the script.
   * Store the cmake script in the core/tests/dir where dir corresponds to the core/apps/dir where the program source resides. Name the file after the test that is being run. 
1. Add any required source or reference data to the appropriate data directory. See Testing Data section for more detail.
1. Any outputs created should be named for the test that creates them. For instance, if a test, called rinex\_creator, creates three files (one for each of the rinex filetypes), then the files should be named something like rinex\_creator.robs, rinex\_creator.rnav, and rinex\_creator.rmet. 

Tips for Writing Application Tests
----------------------------------
* Ensure any output files that are created are given unique names. CTest will run the tests in parallel creating a race condition if applications have the same named output.
* If a test needs output from another test, then specify the dependency by using the set\_tests\_properties CMake command in the CMakeLists.txt file. This would look something like set\_tests\_properties( test1 PROPERTIES DEPENDS test2 ).
* If a test needs multiple commands, they can be strung together with COMMAND statements in one execute\_process. 

Testing Data
------------
Any input data should be placed in the gpstk/data/inputs/ directory and follow this naming convention:
   * File name should describe the key content of the data. For example, a v2.11 Rinex Obs file from day 360 of 2015 could be called robs.v2\_11.doy360.yr2015. 
   * A file taken from a production system may keep its original name if the file is unchanged. 
   * Any inputs generated from existing input files should have the modification description appended to it. Using the example Rinex obs file as a base, a new input with only the first half of that day's data could be called robs.v2\_11.doy360.yr2015.firsthalfday.

Any expected result data should be placed in the gpstk/data/expected/ directory and follow this naming convention:
   * File name be the test's name and end with a .exp. For example, expected output for a test called Foo\_bar should be named Foo\_bar.exp.

Also, any further sub-organization of the testing data is left to the developer's discretion, but make sure that the name of the grouped content is clear. For example, if testing an app called foo required one of each of the Rinex filetypes for a single day, all of the data could be grouped into a directory named for the day.



Examples
--------
### Application Test Example
This illustrates one test on the rmwcheck application. The test is to verify that the application will fail when a non Rinex Met file is given. 
It requires one file to be in the gpstk/data directories, arlm200a.15n.
#### gpstk/ext/tests/checktools/CMakeLists.txt:
This file is where the test scripts parameters are set and the test is added to the CTest suite.
```
...
# check a valid RINEX Nav file (should fail, as it isn't a Met file)
add_test(NAME rmwcheck_Invalid_1
         COMMAND ${CMAKE_COMMAND}
         -DTEST_PROG=$<TARGET_FILE:rmwcheck>
         -DSOURCEDIR=${SD}
         -DTARGETDIR=${TD}
         -DNODIFF=1
         -DARGS=${SD}/arlm200a.15n
         -DGPSTK_BINDIR=${GPSTK_BINDIR}
         -P ${CMAKE_CURRENT_SOURCE_DIR}/../testfailexp.cmake)
...
```

#### gpstk/core/tests/testfailexp.cmake:
This is the script that is run to execute the test.
```
# Generic test where failure is expected
# stick a space-separated argument list in ARGS


# Convert ARGS into a cmake list
IF(DEFINED ARGS)
   string(REPLACE " " ";" ARG_LIST ${ARGS})
ENDIF(DEFINED ARGS)

execute_process(COMMAND ${TEST_PROG} ${ARG_LIST}
                OUTPUT_QUIET
                ERROR_QUIET
                RESULT_VARIABLE HAD_ERROR)
if(HAD_ERROR EQUAL 0)
    message(FATAL_ERROR "Test failed")
endif()

if (HAD_ERROR STREQUAL "Segmentation fault")
    message(FATAL_ERROR "Test had a seg fault")
endif()
```



### Class Unit Test Example
These files illustrate how library unit tests are added to the system. The example shown will be how the ValidType class's tests are created and run.
#### gpstk/core/tests/Utilities/CMakelists.txt:
This snippet creates the executable to run the tests, links the executable to the gpstk library, and adds the executable to be run as part of the test suite.
```
...
add_executable(ValidType_T ValidType_T.cpp)
target_link_libraries(ValidType_T gpstk)
add_test(Utilities_ValidType ValidType_T)
...
```

#### gpstk/core/tests/Utilities/ValidType_T.cpp:
This is the program where the individual unit tests are stored. The file is broken into two parts, the test class and a main segment which instantiates the test class and runs its methods. Each of the test class's methods, shown below, contain multiple tests of a broader topic. For instance methodTest exercises ValidType's methods.
```c++
#include "ValidType.hpp"
#include "TestUtil.hpp"
#include <iostream>
#include <string>
#include <sstream>
#include <cmath>

class ValidType_T
{
public: 
   ValidType_T(){ eps = 1E-12;}// Default Constructor, set the precision value
   ~ValidType_T() {} // Default Desructor

   int methodTest(void)
   {
      TestUtil testFramework( "ValidType", "Various Methods", __FILE__, __LINE__ );
      std::string failMesg;

      gpstk::ValidType<float> vfloat0;

      failMesg = "Is the invalid Valid object set as valid?";
      testFramework.assert(!vfloat0.is_valid(), failMesg, __LINE__);

      failMesg = "Is the invalid Valid object's value 0?";
      testFramework.assert(std::abs(vfloat0.get_value()) < eps, failMesg, __LINE__);

      gpstk::ValidType<float> vfloat (5);

      failMesg = "Does the get_value method return the correct value?";
      testFramework.assert(vfloat.get_value() == 5, failMesg, __LINE__);

      failMesg = "Is the valid Valid object set as valid?";
      testFramework.assert(vfloat.is_valid(), failMesg, __LINE__);

      vfloat.set_valid(false);

      failMesg = "Was the valid Valid object correctly set to invalid?";
      testFramework.assert(!vfloat.is_valid(), failMesg, __LINE__);

      return testFramework.countFails();
   }

   int operatorTest(void)
   {
      TestUtil testFramework( "ValidType", "== Operator", __FILE__, __LINE__ );
      std::string failMesg;

      gpstk::ValidType<float> Compare1 (6.);
      gpstk::ValidType<float> Compare2 (6.);
      gpstk::ValidType<float> Compare3 (8.);
      gpstk::ValidType<int> Compare4 (6);
      gpstk::ValidType<float> vfloat;

      failMesg = "Are two equvalent objects equal?";
      testFramework.assert(Compare1 == Compare2, failMesg, __LINE__);

      failMesg = "Are two non-equvalent objects equal?";
      testFramework.assert(Compare1 != Compare3, failMesg, __LINE__);

      vfloat = 7.;

      testFramework.changeSourceMethod("= Operator");
      failMesg = "Did the = operator store the value correctly?";
      testFramework.assert(vfloat.get_value() == 7., failMesg, __LINE__);

      failMesg = "Did the = operator set the object as valid?";
      testFramework.assert(vfloat.is_valid(), failMesg, __LINE__);

      testFramework.changeSourceMethod("+= Operator");

      vfloat += 3.;
      failMesg = "Did the += operator store the value correctly?";
      testFramework.assert(vfloat.get_value() == 10., failMesg, __LINE__);

      failMesg = "Did the += operator change the object's valid bool?";
      testFramework.assert(vfloat.is_valid(), failMesg, __LINE__);		

      testFramework.changeSourceMethod("-= Operator");

      vfloat -= 5.;

      failMesg = "Did the -= operator store the value correctly?";
      testFramework.assert(vfloat.get_value() == 5., failMesg, __LINE__);

      failMesg = "Did the -= operator change the object's valid bool?";
      testFramework.assert(vfloat.is_valid(), failMesg, __LINE__);

      testFramework.changeSourceMethod("<< Operator");

      vfloat = 11;

      std::stringstream streamOutput;
      std::string stringOutput;
      std::string stringCompare;

      streamOutput <<  vfloat;
      stringOutput = streamOutput.str();

      stringCompare = "11";

      failMesg = "Did the << operator ouput valid object correctly?";
      testFramework.assert(stringCompare == stringOutput, failMesg, __LINE__);

      streamOutput.str("");	// Resetting stream
      vfloat.set_valid(false);

      streamOutput << vfloat;
      stringOutput = streamOutput.str();

      stringCompare = "Unknown";

      failMesg = " Did the << operator output invalid object correctly?";
      testFramework.assert(stringCompare == stringOutput, failMesg, __LINE__);

      return testFramework.countFails();
   }

private:
   double eps;
};

int main() //Main function to initialize and run all tests above
{
	int check = 0, errorCounter = 0;
	ValidType_T testClass;

	check = testClass.methodTest();
	errorCounter += check;

	check = testClass.operatorTest();
	errorCounter += check;

	std::cout << "Total Failures for " << __FILE__ << ": " << errorCounter << std::endl;

	return errorCounter; //Return the total number of errors
}
```
